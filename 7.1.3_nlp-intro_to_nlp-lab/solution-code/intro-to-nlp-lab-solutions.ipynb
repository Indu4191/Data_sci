{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Natural Language Processing Lab\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab we will further explore sklearn and NLTK's capabilities for processing text. We will use the 20 Newsgroup dataset, which is provided by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard Data Science Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting that SKLearn Dataset\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the `fetch_20newsgroups` function to download a training and testing set.\n",
    "\n",
    "Look up the function documentation for how to grab the data.\n",
    "\n",
    "You should pull these categories:\n",
    "- `alt.atheism`\n",
    "- `talk.religion.misc`\n",
    "- `comp.graphics`\n",
    "- `sci.space`\n",
    "\n",
    "Also remove the headers, footers, and quotes using the `remove` keyword argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extracting Information from the Data's Dictionary format \n",
    "# Categories of emails we want\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Setting out training data\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "# Setting our testing data\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Because this is an sklearn dataset, it comes with pre-split train and test sets (note we were able to call 'train' and 'test' in subset).\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description', 'DESCR', 'filenames', 'target_names', 'data', 'target']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure our  Data and Target columns are equal length\n",
    "len(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets checkmeowt what our data actually looks like.\n",
    "data_train['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer.\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data\n",
    "- how big is the feature dictionary?\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- evaluate the performance of a Logistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "\n",
    "**BONUS:**\n",
    "- try a couple modifications:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the target variable look like\n",
    "data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLP Using a count vectorizer.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the vectorizer just like we would set a model\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "# Fitting the vectorizer on our training data\n",
    "cvec.fit(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the length of our data that is in a vectorized state\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26576"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use the stop_words argument to remove words like \"and, the, a\"\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit our vectorizer using our train data\n",
    "cvec.fit(data_train['data'])\n",
    "\n",
    "# and check out the length of the vectorized data after\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforming our x_train data using our fit cvec.\n",
    "# And converting the result to a DataFrame.\n",
    "X_train = pd.DataFrame(cvec.transform(data_train['data']).todense(),\n",
    "                       columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We still have the same number of rows but the vectorization has converted every word, \n",
    "# or what is believed to be a word, from our test data into a feature.  Like dummy coded\n",
    "# variables for words (except counts rather than just occurances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space       1061\n",
       "people       793\n",
       "god          745\n",
       "don          730\n",
       "like         682\n",
       "just         675\n",
       "does         600\n",
       "know         592\n",
       "think        584\n",
       "time         546\n",
       "image        534\n",
       "edu          501\n",
       "use          468\n",
       "good         449\n",
       "data         444\n",
       "nasa         419\n",
       "graphics     414\n",
       "jesus        411\n",
       "say          409\n",
       "way          387\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words appear the most?\n",
    "word_counts = X_train.sum(axis=0)\n",
    "word_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data_train['target_names']\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What are we trying to predict\n",
    "y_train = data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism most common words\n",
      "god         405\n",
      "people      330\n",
      "don         262\n",
      "think       215\n",
      "just        209\n",
      "does        207\n",
      "atheism     199\n",
      "say         174\n",
      "believe     163\n",
      "like        162\n",
      "atheists    162\n",
      "religion    156\n",
      "jesus       155\n",
      "know        154\n",
      "argument    148\n",
      "time        135\n",
      "said        131\n",
      "true        131\n",
      "bible       121\n",
      "way         120\n",
      "dtype: int64\n",
      "\n",
      "comp.graphics most common words\n",
      "image        484\n",
      "graphics     410\n",
      "edu          297\n",
      "jpeg         267\n",
      "file         265\n",
      "use          225\n",
      "data         219\n",
      "files        217\n",
      "images       212\n",
      "software     212\n",
      "program      199\n",
      "ftp          189\n",
      "available    185\n",
      "format       178\n",
      "color        174\n",
      "like         167\n",
      "know         165\n",
      "pub          161\n",
      "gif          160\n",
      "does         157\n",
      "dtype: int64\n",
      "\n",
      "sci.space most common words\n",
      "space        989\n",
      "nasa         374\n",
      "launch       267\n",
      "earth        222\n",
      "like         222\n",
      "data         216\n",
      "orbit        201\n",
      "time         197\n",
      "shuttle      192\n",
      "just         189\n",
      "satellite    187\n",
      "lunar        182\n",
      "moon         168\n",
      "new          158\n",
      "program      156\n",
      "don          151\n",
      "year         146\n",
      "people       142\n",
      "mission      141\n",
      "use          134\n",
      "dtype: int64\n",
      "\n",
      "talk.religion.misc most common words\n",
      "god          329\n",
      "people       267\n",
      "jesus        256\n",
      "don          162\n",
      "bible        160\n",
      "just         159\n",
      "think        151\n",
      "christian    151\n",
      "say          149\n",
      "know         149\n",
      "does         147\n",
      "did          132\n",
      "like         131\n",
      "good         131\n",
      "life         118\n",
      "way          118\n",
      "believe      117\n",
      "said         103\n",
      "point        101\n",
      "time          99\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets look through some of the categories common words\n",
    "common_words = []\n",
    "for i in xrange(4):\n",
    "    word_count = X_train[y_train==i].sum(axis=0)\n",
    "    print names[i], \"most common words\"\n",
    "    cw = word_count.sort_values(ascending = False).head(20)\n",
    "    print cw\n",
    "    common_words.extend(cw.index)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting out vectorized test data to a dataframe\n",
    "# Using the CVEC which we fit earlier\n",
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting our Y test information\n",
    "y_test = data_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74501108647450109"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import and fit our logistic regression and test it too\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hashing and TF-IDF\n",
    "\n",
    "Let's see if Hashing or TF-IDF improves the accuracy.\n",
    "\n",
    "1. Initialize a HashingVectorizer and repeat the test with no restriction on the number of features\n",
    "- does the score improve with respect to the count vectorizer?\n",
    "- print out the number of features for this model\n",
    "- Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "- print out the number of features for this model\n",
    "\n",
    "**BONUS:**\n",
    "- Change the parameters of either (or both!) models to improve your score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743532889874\n",
      "Number of features: 65536\n"
     ]
    }
   ],
   "source": [
    "# A pipeline is a way for us to construct a function to execute\n",
    "# the same tasks continuously\n",
    "# In our variable model we fit a vectorizer, and a model\n",
    "# our Model variable is stored with the fit vectorizer and model\n",
    "# so we we call model.xxxx it uses that information stored\n",
    "model = make_pipeline(HashingVectorizer(stop_words='english',\n",
    "                                        non_negative=True,\n",
    "                                        n_features=2**16),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "print \"Number of features:\", 2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728011825573\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(TfidfVectorizer(stop_words='english',\n",
    "                                      sublinear_tf=True,\n",
    "                                      max_df=0.5,\n",
    "                                      max_features=1000),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print accuracy_score(y_test, y_pred)\n",
    "print \"Number of features:\", len(model.steps[0][1].get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
