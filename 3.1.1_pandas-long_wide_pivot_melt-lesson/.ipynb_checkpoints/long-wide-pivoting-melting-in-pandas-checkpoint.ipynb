{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Pandas Long Format, Wide Format, Pivot Tables, and Melting\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "This lesson is all about data transformation using pandas. Data transformation is the reorganization of the rows and columns of your dataset into a different, potentially more useful shape and format. \n",
    "\n",
    "The benefits of transforming your data are to have better access to relevant information and to make manipulations of that data more streamlined. As you become more familiar with datasets and operations on them you will grow an intuition and appreciation for when operating row-wise or column-wise on your data is preferable.\n",
    "\n",
    "Different data formats are better for different tasks. This takes time and experience to learn, but serves as an introduction to common structures, transformation, and how to apply these transformations.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the differences between long and wide format data\n",
    "- Understand pivot tables\n",
    "- Practice transforming data between long and wide format\n",
    "- Practice creating pivot tables \n",
    "- Learn how to avoid common pitfalls and obstacles in data transformation with pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Wide format data](#wide_format)\n",
    "- [Load and examine the NPAS data](#load_nerdy)\n",
    "- [Long format data](#long_format)\n",
    "- [Using pandas `pivot_table()`: long to wide format](#pivot_tables)\n",
    "- [Multiindex/Hierarchical indices in pandas](#multiindex)\n",
    "- [Using pandas `melt()`: wide to long format](#melt)\n",
    "- [Summarizing data with `pivot_table()`](#pivot_table_summarizing)\n",
    "- [Inner-workings of the multiindex](#examining_multiindex)\n",
    "- [Getting rid of hierarchical indices: \"flattening\" data](#multiindex_to_flat)\n",
    "- [Merging dataframes: long vs. wide](#pandas_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wide_format'></a>\n",
    "\n",
    "### \"Wide\" format data\n",
    "\n",
    "---\n",
    "\n",
    "Between \"wide\" and \"long\", wide format data is the more intuitive one. It is a common format for .csv type files. You have already viewed multiple datasets in wide format throughout this class.\n",
    "\n",
    "Wide format data is formatted so that:\n",
    "\n",
    "- Unique IDs, subjects, observations, etc. are represented as rows.\n",
    "- Distinct information categories (variables) are represented as columns. In other words, there is a column for every \"variable\" with its own unique values.\n",
    "- The format can often be a more compact matrix, particularly if no or little information is missing.\n",
    "- Is not as useful for SQL-style operations: it can make it much harder or even impossible to join tables together on a value.\n",
    "- Wide can be more useful in pandas when you need to preform operations on variables **across columns**. For example, multiplying columns together to create a new column.\n",
    "- It is the data format required for statistical modeling (with few exceptions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_nerdy'></a>\n",
    "\n",
    "### Load the \"Nerdy Personality Attributes\" dataset\n",
    "\n",
    "---\n",
    "\n",
    "This is a pre-cleaned and modified version of the full \"Nerdy Personality Attributes\" survey that asked subjects to self-rate on questions related to \"nerdiness\" as well as more general personality traits such as openness and extraversion. Demographic information on the subjects was also collected.\n",
    "\n",
    "[You can find the raw data here along with many other sociological surveys.](http://personality-testing.info/_rawdata/)\n",
    "\n",
    "In this modified version, for the sake of example, some of the subjects have data for the survey but not the demographic variables. Because there are missing values and the data is \"messy\", we have a data cleaning problem.\n",
    "\n",
    "**Load the data (which is in wide format).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_wide_f = './datasets/NPAS_parsed_trunc_wide_missing.csv'\n",
    "\n",
    "# load data and print the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in a familiar format where each column is a variable and each row contains the observation for that variable which corresponding to a distinct subject.\n",
    "\n",
    "*Wide format implies that all of the information for one distinct subject will be represented in the columns corresponding to that row. A single subject should not have multiple rows of data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check to see how many null values there are per column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count null values by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 691 missing demographic variables are intentional (I specifically enforced that only 700 of the subjects would have demographic information).\n",
    "\n",
    "However, we can see that the `major` variable has 970 missing values. This was not an intentional change.\n",
    "\n",
    "If we were to just drop all the rows that have any null values at this point, we would lose 970 rows due to the commonly missing variable `major`.\n",
    "\n",
    "With a numeric column, this would be hard to avoid without \"imputing\" some number to fill in the values. In the simplest case imputing the mean or median for missing numeric values is commonly used (but not ideal).\n",
    "\n",
    "With a **categorical variable**, which `major` is, we have the luxury of replacing the missing values with a new category label that stands for \"missing\". \n",
    "\n",
    "**Replace the missing major column values with \"unknown\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set missing values in major to \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='long_format'></a>\n",
    "\n",
    "### \"Long\" format data\n",
    "\n",
    "---\n",
    "\n",
    "Now we can load the same data but instead in the format commonly called \"long\".\n",
    "\n",
    "Long data is formatted so that:\n",
    "\n",
    "- There are potentially multiple \"id\" (identification) columns.\n",
    "- There are pairs of columns such as `variable:value` that match a variable key to a value (in the simple case, there would be a single `variable` column and a single `value` column).\n",
    "- The \"variable\" column corresponds to the multiple variable columns in a wide format dataset. Instead of a column for each variable like in wide, you have a row for each variable:value pair, *per id*. \n",
    "- This is a standard format for SQL databases because it makes joining different tables together by keys easier.\n",
    "\n",
    "**Load the long format of the same data below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nerdy_long_f = './datasets/NPAS_parsed_trunc_long_missing.csv'\n",
    "\n",
    "# load long data and print the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the long data has far more rows than the wide dataset, but only three columns.\n",
    "\n",
    "Below you can view the three columns: `subject_id`, `variable`, and `value`.\n",
    "\n",
    "**`subject_id:`**\n",
    "- This is the primary \"key\" or \"id\" column. Each subject id will have corresponding entries in the variable column, one for each row.\n",
    "\n",
    "**`variable:`**\n",
    "- This column indicates which variable the item in the value column corresponds to.\n",
    "\n",
    "**`value:`**\n",
    "\n",
    "- This contains all the values for all of the variables for all ids. Essentially, every cell in the wide dataset except the subject_id is listed in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the unique values in the `variable` column.**\n",
    "\n",
    "You can see that the unique values in the variable column correspond to the column headers of the wide format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the unique values in the variable column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the unique subject ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace the missing values in major with \"unkown\" in the long format dataset.**\n",
    "\n",
    "The process for replacing the data will be different due to the format. Using logical selection masks with pandas `.loc` syntax is the preferable way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the missing values for major in the long dataset with \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pivot_tables'></a>\n",
    "\n",
    "### Pandas `pivot_table()`: going from long to wide format\n",
    "\n",
    "---\n",
    "\n",
    "The `pd.pivot_table()` function is a very powerful tool to both transform data from long to wide format as well as summarize data with user-supplied functions.\n",
    "\n",
    "First we'll look at transforming the long format data back into the wide format using the `pivot_table` function.\n",
    "\n",
    "**Important parameters for the `pivot_table` function:**\n",
    "\n",
    "    nerdy_long: the pivot_table() function takes a dataframe to pivot as its first argument\n",
    "    \n",
    "- **`columns`**: this is the list of columns in the wide format data to transform back to columns in wide format, with each unique value in the long format column becoming a header for the wide format.\n",
    "- **`values`**: a single column indicating the values to use when pivoting and filling in the new wide format columns.\n",
    "- **`index`**: columns in the long format data that are index variables – this means that these will be left as single columns, not spread out across columns by unique value such as in the columns parameter .\n",
    "- **`aggfunc`**: often pivot_table() is used to perform a summary of the data. aggfunc stands for \"aggregation function\". It is required and defaults to np.mean. You can put your own function in, which I do below.\n",
    "- **`fill_value`**: if a cell is missing for the wide format data, the value to fill in.\n",
    "    \n",
    "Below we put in our own function `select_item_or_nan()` to the `aggfunc` keyword argument. Because my `subject_id` column has a single variable value for each id, I just want the single element in the long format value cell. My data is messy and so I have to write a function to check for places it can break. \n",
    "\n",
    "Note: `x` passed into my function will be a series object. I pull out the first element of that with the `.iloc` indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def select_item_or_nan(x):\n",
    "#     x = x.iloc[0]\n",
    "#     if len(x) == 0:\n",
    "#         return np.nan\n",
    "#     else:\n",
    "#         return x\n",
    "\n",
    "# nerdy_wide = pd.pivot_table(nerdy_long, columns=['variable'], values='value',\n",
    "#                             index=['subject_id'], aggfunc=select_item_or_nan,\n",
    "#                             fill_value=np.nan)\n",
    "\n",
    "# nerdy_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiindex'></a>\n",
    "\n",
    "### Multiindex/Hierarchical indexing\n",
    "\n",
    "---\n",
    "\n",
    "In the header you can see that the format of the wide data is *not* the same as our original loaded wide format. Pandas implements something called **Multiindexing** or **Hierarchical indexing** which allows for \"tiered\" row and column labels.\n",
    "\n",
    "Right now the multiindex is not terrible, but this can get very confusing and annoying, which we will see further down in the lesson.\n",
    "\n",
    "The main difference is that we have a `variable` name in the top left corner, which is \"labeling\" our columns (and corresponds to the name of our original column in the long format data). The row indexer has become our single key/id variable `subject_id`. The columns are what we would expect here, each one a variable like in the original wide data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the header of the widened dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the null values from our recreated wide data. How many unique subjects do we have?**\n",
    "\n",
    "Remember our `subject_id` is now the **index**, and so we can access it with the `.index` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop the null values and count unique subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the `subject_id` index back into a column.**\n",
    "\n",
    "We can use the dataframe function `.reset_index()` to move `subject_id` into a column and create a new index. Now we have the dataframe in the format we got when we loaded the original wide data in before. The only exception is that we still have that \"variable\" column label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert the index to a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the label of the columns.**\n",
    "\n",
    "You can remove the column label (which can be confusing during print statements) by setting the `.columns.name` attribute to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the columns label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='melt'></a>\n",
    "\n",
    "### Going from wide to long with `.melt()`\n",
    "\n",
    "---\n",
    "\n",
    "**`.melt()`** is a function that essentially performs the inverse operation of `pivot_table` on dataframes.\n",
    "\n",
    "Melt takes a dataframe as its first argument. Additional arguments typically used in the melt function are:\n",
    "\n",
    "- **`id_vars`**: the column or columns that will be id variables. id variables contain datapoints specified by the variable and value columns.\n",
    "- **`value_vars`**: a list that specifies which columns should be converted into a single value column and variable column.\n",
    "- **`var_name`**: the header name of the variable column (default='variable').\n",
    "- **`value_name`**: the header name of the value column (default='value').\n",
    "\n",
    "**First, subset the WIDE data to just columns: `['subject_id','anxious','booking','calm','major']`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset the wide data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `melt` on the subset with `id_vars=['subject_id','major']`.**\n",
    "\n",
    "Print out the shape of the data and the header. The non-id columns and their values are now represented by the variable:value column pair.\n",
    "\n",
    "Note: when you only specify the `id_vars`, the remaining columns are inferred to become part of the variable and value columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_sub_long = pd.melt(nerdy_subset, id_vars=['subject_id','major'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same thing as above without having to subset the dataframe first by simply specifying the `value_vars` keyword argument. The output dataframe will then only contain the data specified in the `id_vars` and `value_vars` arguments.\n",
    "\n",
    "**Create the same dataframe with melt on the full wide dataset, but select the columns to use with the `value_vars` argument.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_sub_long = pd.melt(nerdy_wide_flat, id_vars=['subject_id','major'], \n",
    "#                          value_vars=['anxious','bookish','calm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value column is still a string, so we can convert it to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure the value is a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pivot_table_summarizing'></a>\n",
    "\n",
    "### Summarizing your data with  `pivot_table` and aggregate functions\n",
    "\n",
    "---\n",
    "\n",
    "For those of you experienced with Excel, the pandas pivot table does the same thing as the pivot table in Excel. It's more powerful, but harder to use than the user-friendly spreadsheet version.\n",
    "\n",
    "Pivot table can take in a variable, value, and an index to group by and apply aggregate functions to summarizing the data. \n",
    "\n",
    "Note: be careful that your index variable is not be pulling out unique rows (for example, subject_id by variable would only have one value to send into the aggregate functions).\n",
    "\n",
    "Below I am calling the `pivot_table` function with:\n",
    "- the long format data as the first argument\n",
    "- \"variable\" specified as the columns that indicate the variable names (groups)\n",
    "- \"value\" specified to be the column that contains the data per variable\n",
    "- \"major\" as the index: the rows will be grouped by major\n",
    "- `np.mean`, `np.median`, `np.std`, and `len` as aggregate functions. These will be calculated for each major-by-variable group\n",
    "- a fill value of `np.nan`, for cells in the output table that have no data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_major_summary = pd.pivot_table(nerdy_sub_long, columns=['variable'], values='value',\n",
    "#                                      index=['major'], aggfunc=[np.mean, np.median, np.std, len],\n",
    "#                                      fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataframe gives you a \"hierarchical\" column index – the three variable for each aggregate function. The row index is the \"major\" groups.\n",
    "\n",
    "If you apply more index variables to split by, the row indices will also become hierarchical! It can get bloated fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the header of the pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='examining_multiindex'></a>\n",
    "\n",
    "### Inner-workings of the the multiindex\n",
    "\n",
    "--- \n",
    "\n",
    "The `.names` attribute on the index and the columns will show you the hierarchy of labels. The row index is \"major\", and the two column indices are None and 'variable' (the aggregate functions get no label from pivot table in this case). \n",
    "\n",
    "If you print out the columns, you can see it has become a pandas `MultiIndex` object that has levels, labels, and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print nerdy_major_summary.index.names\n",
    "# print nerdy_major_summary.columns.names\n",
    "# print nerdy_major_summary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing along the hierarchical column headers can be done with chained bracket keys, with the top level column label in the first bracket down to the bottom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_major_summary['mean'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_major_summary['mean']['anxious'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_major_summary['mean'][['anxious','bookish']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you can just split them up by comma within the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_major_summary['mean', 'bookish'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiindex_to_flat'></a>\n",
    "\n",
    "### Getting rid of the Multiindex: converting back to \"flat\" data\n",
    "\n",
    "---\n",
    "\n",
    "Multiindex dataframes have great potential use and are a cool concept. That being said, the overhead and confusion on how to subset/mask them is most often not worth it, especially when your data needs to be formatted for insertion into a model.\n",
    "\n",
    "The most reliable way to \"flatten\" a multi-indexed dataframe is through the `.to_records()` function. To make this a new dataframe, it needs to be wrapped in a `pd.DataFrame()` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nerdy_major_flat = pd.DataFrame(nerdy_major_summary.to_records())\n",
    "# nerdy_major_flat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the new column names are tuples of the hierarchy of the multiindexed columns. You can convert these to new, more easily indexed columns with something like a list comprehension, for example.\n",
    "\n",
    "The **`eval`** function takes a string and trys to evaluate it as if it were a python command.\n",
    "\n",
    "**Use a list comprehension and the `eval` function to convert the flattened multiindex columns to something more readable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the column names with list comprehension and eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merging_joining_preface'></a>\n",
    "\n",
    "### Preface to merging and joining with long and wide data\n",
    "\n",
    "---\n",
    "\n",
    "You will be merging and joining datasets extensively throughout this course and your future careers. It is important to note the differences between merging long and wide datasets together.\n",
    "\n",
    "**Load in the data used above, but now split so that the demographic variables are in one dataset and the survey question answers are in another another.** \n",
    "\n",
    "These datasets are in wide format, and they both contain `subject_id` to identify who the questions are for. \n",
    "\n",
    "As you may recall, the demographic responses have fewer observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_demos_file = './datasets/NPAS_parsed_trunc_demo_sample.csv'\n",
    "n_survey_file = './datasets/NPAS_parsed_trunc_survey.csv'\n",
    "\n",
    "# load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the header of the demos and survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas_merge'></a>\n",
    "\n",
    "### Pandas `.merge()` function: joining long format vs. wide format data\n",
    "\n",
    "---\n",
    "\n",
    "The merge function is a built-in function in a DataFrame. The first argument is another DataFrame that you want to merge it with, and the `on` keyword argument is the key or keys that you want the DataFrames to be \"matched\" on.\n",
    "\n",
    "We are specifying `how='inner'` here, which means that the key must be present in both dataframes have that row present in the output. Because the demographics dataset has fewer subject_ids, it will only merge the subject_id rows from the survey dataset that are present in the demographics dataset.\n",
    "\n",
    "**Merge the survey and demographic wide-format datasets together using `.merge()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demos_survey = demos_subset.merge(survey, on=['subject_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the merged data header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the demographic and survey data into long format using `melt()`.**\n",
    "\n",
    "- For the demographic dataframe, specify two id_vars, gender and subject_id.\n",
    "- For the survey dataframe, only specify subject_id for id_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# melt the demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# melt the survey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge together the long form datasets just like we did before with the wide format data.**\n",
    "\n",
    "Here we will still merge on 'subject_id' with 'inner' for the how variable. We have duplicate named columns in each of these dataframes ('variable' and 'value'). We can specify `suffixes=('_survey','_demo')` to give the instances of the survey and demographic dataframes appropriate column names when they are joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge the survey and demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
