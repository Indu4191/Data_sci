{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Web Scraping and Spiders with `scrapy`\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the structure and content of HTML\n",
    "- Learn about elements, attributes, and element hierarchy in HTML\n",
    "- Learn about XPath and using multiple and singular selections\n",
    "- Practice using Scrapy to get data from craigslist\n",
    "- Practice using Beautiful Soup to parse data from craigslist\n",
    "- Walkthrough the construction of a spider built using scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#introduction)\n",
    "- [HTML](#html)\n",
    "    - [Elements](#elements)\n",
    "    - [Attributes](#attributes)\n",
    "    - [Element hierarchy](#element-hierarchy)\n",
    "    - [More resources on HTML structure](#html-resources)\n",
    "- [What is XPath?](#xpath)\n",
    "    - [Multiple selections](#multiple-selections)\n",
    "    - [Singular selections](#singular-selections)\n",
    "- [A simple `scrapy` example](#scrapy)\n",
    "- [A practical example with Requests and Beautiful Soup](#practical)\n",
    "    - [Step 1: fetch the content by URL](#step1)\n",
    "    - [Step 2: Parse HTML document with Beautiful Soup](#step2)\n",
    "    - [Practice: can you select the price of our junker?]\n",
    "- [Scrapy and spiders](#scrapy-spiders)\n",
    "    - [Create a Scrapy project](#scrapy-project)\n",
    "    - [Define an \"item\"](#define-item)\n",
    "    - [A spider that crawls](#spider-crawl)\n",
    "    - [XPath and parsing with our spider](#xpath-spider)\n",
    "    - [Save and examine our scraped data](#save-examine)\n",
    "- [Addendum: leveraging XPath to get more results](#addendum)\n",
    "    - [Following links](#follow-links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "![What is Html](http://designshack.designshack.netdna-cdn.com/wp-content/uploads/htmlbasics-0.jpg)\n",
    "\n",
    "One of the largest sources of data in the world is all around us.  We consume the web in some form every day.  One of the most powerful python toolsets we will learn allows us to extract and normalize data from unstructured sources like webpages.  \n",
    "\n",
    "**If you can see it, it can be scraped, mined, and put into a dataframe.**\n",
    "\n",
    "Before we begin the actual process of webscraping with python, it is important to cover the basic constructs that describe HTML as unstructured data. \n",
    "\n",
    "Then we will cover a a powerful selection technique called XPath, and look at a basic workflow using a framework called [Scrapy](http://www.scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html'></a>\n",
    "\n",
    "## Hypertext markup language (HTML)\n",
    "\n",
    "---\n",
    "\n",
    "In the HTML DOM (Document Object Model), everything is a node:\n",
    " * The document itself is a document node.\n",
    " * All HTML elements are element nodes.\n",
    " * All HTML attributes are attribute nodes.\n",
    " * Text inside HTML elements are text nodes.\n",
    " * Comments are comment nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elements'></a>\n",
    "### Elements\n",
    "Elements begin and end with open and close \"tags\", which are defined by namespaced, encapsulated strings. \n",
    "\n",
    "```\n",
    "<title>I am a title.</title>\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "\n",
    "**Elements begin and end in the same namespace like so:**  `<p></p>`\n",
    "\n",
    "**Elements can have parents and children:**\n",
    "\n",
    "```\n",
    "<body>\n",
    "    <div>I am inside the parent element\n",
    "        <div>I am inside a child element</div>\n",
    "        <div>I am inside another child element</div>\n",
    "        <div>I am inside yet another child element</div>\n",
    "    </div>\n",
    "</body>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>\n",
    "### Attributes\n",
    "\n",
    "HTML elements can have attributes.  They describe properties, and characteristics of elements.  Some affect how the element behaves or looks in terms of the rendered output by the browser.\n",
    "\n",
    "The most common element is an \"anchor\" element.  Anchor elements often have an \"href\" element, which tells the browser where to go after it is clicked.  Anchor elements are typically are formatted in bold, and sometimes are underlined as a visual cue to differentiate itself.\n",
    "\n",
    "**Markup that describes nn element with attributes, litterally looks like this**\n",
    "\n",
    "```\n",
    "<a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">An Awesome Website</a>\n",
    "```\n",
    "\n",
    "**However, this element, once rendered, looks like this**\n",
    "\n",
    "[An Awesome Website](https://www.youtube.com/watch?v=dQw4w9WgXcQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='element-hierarchy'></a>\n",
    "### Element hierarchy\n",
    "\n",
    "![Nodes](http://www.computerhope.com/jargon/d/dom1.jpg)\n",
    "\n",
    "**Literally Represented:**\n",
    "\n",
    "```\n",
    "<html>\n",
    "    \n",
    "    <head>\n",
    "        <title>Example</title>\n",
    "    </head>\n",
    "    \n",
    "    <body>\n",
    "        <h1>Example Page</h1>\n",
    "        <p>This is an example page.</p>\n",
    "    </body>\n",
    "    \n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html-resources'></a>\n",
    "### You are now qualified HTML experts\n",
    "\n",
    "![](http://hpcc.advancingexpertcare.org/wp-content/uploads/2014/10/certified.jpg)\n",
    "\n",
    "Your HTML learning can continue...\n",
    "\n",
    "Read all about the different elements supported amongst modern browsers:\n",
    " * [HTML5 Cheatsheet](http://websitesetup.org/html5-cheat-sheet/)\n",
    " * [Mozilla HTML Element Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    " * [HTML5 Visual Cheatsheet](http://www.unitedleather.biz/PDF/HTML5-Visual-Cheat-Sheet1.pdf)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xpath'></a>\n",
    "\n",
    "## What is XPath?\n",
    "\n",
    "---\n",
    "\n",
    "![](http://img.crx4chrome.com/63/4c/b1/hgimnogjllphhhkhlmebbmlgjoejdpjl-screenshot.jpg)\n",
    "\n",
    "Understanding how to identify elements and attributes within HTML documents gives us the capability to write simple expressions that create structured data.\n",
    "\n",
    "To make this process easier to deal with, we will be using XPath helper, which is a Chrome addon.  It's not necessary, but highly recommended to help build XPath expressions.\n",
    "\n",
    "[XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en)\n",
    "\n",
    "XPath expressions can select elements, element attributes, and element text.  These selections can be either to a single item, or multiple items.  Generally, if you're not specific enough, you will end up selecting multiple elements.\n",
    "\n",
    "\n",
    "<a id='multiple-selections'></a>\n",
    "### Multiple selections\n",
    "\n",
    "***Multiple selections*** are useful for capturing search results, or any repeating element.  For instance, the _titles_ of an apartment listing search results from Craigslist:\n",
    "\n",
    "\n",
    "**URL**\n",
    "\n",
    "[http://sfbay.craigslist.org/search/sfc/apa](http://sfbay.craigslist.org/search/sfc/apa)\n",
    "\n",
    "\n",
    "**HTML Markup**\n",
    "```\n",
    "...\n",
    "<span class=\"pl\"> \n",
    "    <time datetime=\"2016-01-12 23:27\" title=\"Tue 12 Jan 11:27:35 PM\">Jan 12</time> \n",
    "    <a href=\"/sfc/apa/5400584579.html\" data-id=\"5400584579\" class=\"hdrlnk\">Welcome home to a sweetly renovated four bedroom one and a half bath</a> \n",
    "</span>\n",
    "...\n",
    "```\n",
    "\n",
    "**XPath - Multiple Titles**\n",
    "```\n",
    "//a[@class='hdrlnk']\n",
    "```\n",
    "\n",
    "**Returns (Ad Titles)**\n",
    "```\n",
    "***New Remodeled two bedroom Apartment***\n",
    "WONDERFUL ONE BR APARTMENT HOME\n",
    "Beautiful 1bed/1bath Apartment in Russian Hill NO SECURITY DEPOSIT\n",
    "Knockout SF View|Green Oasis|Private Driveway|Furnished\n",
    "3BR/3BA Spacious, Beautiful SOMA Loft: 5 month lease\n",
    "Nob Hill Large Studio - Light, Quiet, Lovely Building\n",
    "etc...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='singlular-selections'></a>\n",
    "\n",
    "### Singular selections\n",
    "\n",
    "***Singular selections*** are necessary when you want to grab specific, unique text within elements.  Here's an example of a details page on Craigslist:\n",
    "\n",
    "> *Note: this example may be expired if you view it sometime after Jan 12th, 2016. Please replace this with a current craigslist listing!\n",
    "\n",
    "**URL**\n",
    "\n",
    "(Only $8000!)\n",
    "[http://sfbay.craigslist.org/sfc/apa/5400585892.html](http://sfbay.craigslist.org/sfc/apa/5400585892.html)\n",
    "\n",
    "**HTML Markup**\n",
    "\n",
    "```\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "**XPath - Single Item**\n",
    "\n",
    "```\n",
    "//p[@class='postinginfo'][2]/time\n",
    "```\n",
    "**Returns (Time of posting)**\n",
    "```\n",
    "2016-01-12 11:23pm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "\n",
    "## A simple `scrapy` example\n",
    "\n",
    "---\n",
    "\n",
    "Below is an example of how to get information out of some fake HTML using the XPath capabilities of the `scrapy` package. You will likely need to install the scrapy package using `conda` or `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "HTML = \"\"\"\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# 1 done!!!\n",
    "best = Selector(text=HTML).xpath(\"//span[@class='bestof-text']/text()\").extract()\n",
    "\n",
    "# 2 \n",
    "best = Selector(text=HTML).xpath(\"//span[contains(text(), 'best of')]/text()\").extract()\n",
    "\n",
    "# 3 /html/div/p/@class/a\n",
    "best        =  Selector(text=HTML).xpath(\"/html/body/div/p/a[@class='bestof-link']\")\n",
    "nested_best =  best.xpath(\"./span[@class='bestof-text']/text()\").extract()\n",
    "nested_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a>\n",
    "\n",
    "## A Practical Example with Requests + Beautiful Soup\n",
    "\n",
    "---\n",
    "\n",
    "Please make sure that the required packages are installed: \n",
    "\n",
    "```bash\n",
    "# beautiful soup:\n",
    "> conda install bs4 \n",
    "> conda install lxml\n",
    "\n",
    "# or if conda doesn't work\n",
    "> pip install bs4\n",
    "> pip install lxml\n",
    "```\n",
    "\n",
    "Here's another posting for a sweet ride on Craigslist (as of 04/29/2016):\n",
    "\n",
    "![](http://images.craigslist.org/00x0x_hMg0axS9t35_600x450.jpg)\n",
    "\n",
    "> *Note: you will need to update this to a current/working craigslist post.*\n",
    "\n",
    "https://merced.craigslist.org/cto/6034381423.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: fetch the content by URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First part of HTML document fetched as string:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\">\n",
      "<head>\n",
      "<title>1999 saturn</title>\n",
      "    \t<link rel=\"canonical\" href=\"http://merced.craigslist.org/cto/6034381423.html\">\n",
      "\t<meta name=\"description\" content=\"1999 Saturn. Parts car only. Pick up only. Motor and tranny are in great condition. Brand new tires. Aluminum wheels. $600 or best offer. \">\n",
      "\t<meta name=\"robots\" content=\"noarchive,nofollow,unavailable_after:Thursday, 06-Apr-17 19:59:20 PDT\">\n",
      "\t<meta name=\"twitter:card\" content=\"preview\">\n",
      "\t<meta property=\"og:d\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from lxml import html\n",
    "\n",
    "url = \"https://merced.craigslist.org/cto/6034381423.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Pull HTML string out of requests\n",
    "html = response.text\n",
    "\n",
    "# The first 500 characters of the content\n",
    "print \"\\nFirst part of HTML document fetched as string:\\n\"\n",
    "print html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: Parse HTML document with Beautiful Soup\n",
    "\n",
    "This step allows us to access the elements of the document by XPATH expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** There are many ways to get the elements in a \"soup\" object\n",
    "\n",
    "Here are a few ways to select HMTL elements as \"objects\" within \"soup\" as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>1999 saturn</title>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular element\n",
    "soup.html.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 saturn\n"
     ]
    }
   ],
   "source": [
    "# Just the text between elements\n",
    "print soup.html.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plural / Repeating elements\n",
    "for meta in soup.html.meta.children:\n",
    "    print meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'CL'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find single or multiple elements\n",
    "# First parameter\n",
    "element = soup.findAll(\"a\", {\"class\": \"header-logo\"})\n",
    "element[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'$600'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_search = soup.findAll('span', {\"class\": \"price\"})\n",
    "price_search[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://sfbay.craigslist.org/search/sfc/apa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)\n",
    "search_titles = soup.findAll(\"a\", {\"class\": \"hdrlnk\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data-id': '6046406176', 'href': '/sfc/apa/6046406176.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6038233283', 'href': '/sfc/apa/6038233283.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6026983369', 'href': '/sfc/apa/6026983369.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6030386716', 'href': '/sfc/apa/6030386716.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6046405539', 'href': '/sfc/apa/6046405539.html', 'class': ['result-title', 'hdrlnk']}\n"
     ]
    }
   ],
   "source": [
    "for link in search_titles[0:5]:\n",
    "    print link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Check:** How do we know which parameters `findAll()` takes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "### Practice: can you select the price of our junker?  \n",
    "\n",
    " - Use XPath Helper to get an idea of where the element is within the HTML document.\n",
    " - Try to select using the soup.html.body.something.something method.\n",
    " - Try using findAll() to find a concise element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a scrapy-spiders></a>\n",
    "## What is [Scrapy](http://scrapy.org/)?\n",
    "\n",
    "---\n",
    "\n",
    "> *\"Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\"*\n",
    "\n",
    "Below we will walkthrough the creation of a **spider** using scrapy. Spiders are automated processes that will crawl through a webpage or webpages and collect information.\n",
    "\n",
    "> **Note:** This code should be written in a script outside of jupyter notebook.\n",
    "\n",
    "<a id='scrapy-project'></a>\n",
    "### 1. Create a new Scrapy project\n",
    "\n",
    "> `scrapy startproject craigslist`\n",
    "\n",
    "**Should create output that looks like this:**\n",
    "<blockquote>\n",
    "```\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Overridden settings: {}\n",
    "New Scrapy project 'craigslist' created in:\n",
    "    /Users/davidyerrington/virtualenvs/data/scraping/craigslist\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd craigslist\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**That command generates a set of project files:**\n",
    "<blockquote>\n",
    "```\n",
    "craigslist/\n",
    "    scrapy.cfg\n",
    "    craigslist/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Generally, these are our files.  We will go into more detail on these soon.\n",
    "\n",
    " * **`scrapy.cfg`:** the project configuration file\n",
    " * **`craigslist/`:** the project’s python module, you’ll later import your code from here.\n",
    " * **`craigslist/items.py`:** the project’s items file.\n",
    " * **`craigslist/pipelines.py`:** the project’s pipelines file.\n",
    " * **`craigslist/settings.py`:** the project’s settings file.\n",
    " * **`craigslist/spiders/`:** a directory where you’ll later put your spiders.\n",
    " \n",
    "Long story, but please add this line to your craigslist/settings.py file before continuing:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " ```\n",
    " </blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='define-item'></a>\n",
    "### 2. Define an \"item\"\n",
    "\n",
    "Basically, when we define an item, it's telling our new application what it will be collecting.  In essence, an \"item\", is an entity that has attributes (ie: \"title\", \"description\", \"price\", etc) that are descriptive and relate to elements on pages that we will be scraping.  \n",
    "\n",
    "In more precise terms, this is a model (for those who are familliar with ORM or relational database terms).  Don't worry if this is a foreign concept.  The main idea to understand is that a model has attributes that closely resemble / relate to elements on our target web page(s).\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CraigslistItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='spider-crawl'></a>\n",
    "### 3. A spider that crawls\n",
    "\n",
    "An item is a model that resembles data on a webpage.  A spider is something that crawls pages and uses our item model to to get and hold items for us.\n",
    "\n",
    "**Scrapy spiders are python classes.  Let's write our first file, called `craigslist_spider.py` and put it in our `/spiders` directory:**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = \"craigslist\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "        \"http://sfbay.craigslist.org/search/sfc/apa\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "\n",
    "**Next, let's dive in and crawl from our `/craigslist/craigslist` directory:**\n",
    "\n",
    "```\n",
    "> scrapy crawl craigslist\n",
    "```\n",
    "\n",
    "**What just happened?**\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * Ran parse over the content containing the HTML markup, of each request URL.\n",
    " * What else?\n",
    " \n",
    "```python\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.body)\n",
    "```\n",
    "\n",
    "It saved a file in our base project directory.  It should be named based on the end of the URL.  In our case, it should create a file called \"sfc\".  This is taken directly from the Scrapy docs and it's only point is to illustrate the workflow so far.  It is kind of nice to have a reference to our HTML file though.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='xpath-spider'></a>\n",
    "### 4. XPath + parsing with our spider\n",
    "\n",
    "So far, we've defined what fields we'll get, some urls to fetch, and saved some content to a file.  Let's actually do something interesting.\n",
    "\n",
    "**We should let our spider know about the item model we made earlier.  In the head of the `craigslist/craigslist/spiders/craigslist_spider.py`, lets add a new import:**\n",
    "\n",
    "```python\n",
    "from craigslist.items import CraigslistItem\n",
    "```\n",
    "\n",
    "> **Check:** Why won't it work otherwise?\n",
    "\n",
    "<br><br><br>\n",
    "**Let's replace our parse method, to find some data from our Craigslist spider response, and map it to our item model, CraigslistItem:**\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "\n",
    "    for sel in response.xpath(\"//div[@class='content']/span[@class='rows']/p\"):\n",
    "\n",
    "        item = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"span/span/a[@class='hdrlnk']\").extract()[0]\n",
    "        item['link']  =  sel.xpath(\"span/span/a[@class='hdrlnk']/@href\").extract()[0]\n",
    "        yield item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='save-examine'></a>\n",
    "### Save and examine our scraped data\n",
    "\n",
    "By default, we can save our crawled data as json.  To save our data, we just need to pass an optional parameter to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "> scrapy crawl craigslist -o items.json\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "It's always good to iteratively check our data when developing a spider to make sure it's close to what we want. \n",
    "\n",
    "> *Pro tip:  The longer your iterations are between checks, the harder it's going to be to understand what's no working and fix bugs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# update this path to your own\n",
    "# hint: from terminal, use the pwd command in the same directory as items.json to find\n",
    "# your scraping directory with your json file\n",
    "# pd.read_json(\"/Users/davidyerrington/virtualenvs/data/scraping/craigslist/craigslist/items.json\").head()\n",
    "\n",
    "# df = pd.read_csv(\"/Users/davidyerrington/scrapy_projects/craigslist/craigslist/apts.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addendum'></a>\n",
    "## Addendum: leveraging XPath to get more results\n",
    "\n",
    "---\n",
    "\n",
    "Generally, a workflow that is useful in this context is to load the page in your Chrome browser, check out the page using the XPath Helper plugin, and from that derive your own XPath expressions based on the output.\n",
    "\n",
    "`text()` selects only the text of a given element (between the tags), and `@attribute_name` is used to select attributes.\n",
    "\n",
    "**Here are a few examples of `text()`**:\n",
    "<blockquote>\n",
    "```\n",
    "<h1>Darwin - The Evolution Of An Exhibition</h1>\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "The XPath selector for this:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "//h1/text()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**Here are a few examples of attributes**:\n",
    "\n",
    "And the description is contained inside a `<div>` tag with `id=\"description\"`:\n",
    "<blockquote>\n",
    "```\n",
    "<h2>Description:</h2>\n",
    "\n",
    "<div id=\"description\">\n",
    "Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.\n",
    "</div>\n",
    "...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "XPath\n",
    "<blockquote>\n",
    "```\n",
    "//div[@id='description']\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following links for more results\n",
    "\n",
    "100 results is pretty cool but what if we want more?  We need to follow the \"next\" links, and find new pages to grab.  Using the **`parse()`** method of our spider class, we only need to return another type of object.\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "\n",
    "    for sel in response.xpath(\"//div[@class='content']/span[@class='rows']/p\"):\n",
    "\n",
    "        item = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"span/span/a[@class='hdrlnk']\").extract()[0]\n",
    "        item['link']  =  sel.xpath(\"span/span/a[@class='hdrlnk']/@href\").extract()[0]\n",
    "        item['price'] =  sel.xpath(\"span/span/span[@class='price']\").extract()[0]\n",
    "        yield item\n",
    "\n",
    "    # Does the next page exist?  Let's get it!\n",
    "    next_page   = response.xpath(\"(//a[@class='button next']/@href)[1]\")\n",
    "\n",
    "    if next_page:\n",
    "        url = response.urljoin(next_page[0].extract())\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
