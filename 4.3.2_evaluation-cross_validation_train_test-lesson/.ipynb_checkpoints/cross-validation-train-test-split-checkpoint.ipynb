{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Style and Markdown guide for DSI Lessons/Labs\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Joseph Nelson (DC), Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- **Describe** test/train/split and cross-validation\n",
    "- **Explain** why we want to use these validation techniques and how they differ\n",
    "- **Split** data into testing and training sets using both test/train/split and cross validation and **Apply** both techniques to score a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Overfitting and underfitting](#overfitting-underfitting)\n",
    "- [Train-test split](#train-test-split)\n",
    "- [K-Fold cross-validation](#cross-val-k-fold)\n",
    "- [Train-test-split demonstration](#demo)\n",
    "    - [Plot a heatmap](#heatmap)\n",
    "    - [Select a single predictor for SLR](#single-predictor)\n",
    "    - [Split data into training and testing](#sklearn-tts)\n",
    "    - [Fit on the training data](#fit-on-train)\n",
    "- [K-Fold cross-validation demonstration](#cv-demo)\n",
    "- [Review: negative $R^2$ values](#neg-r2)\n",
    "- [Hold-out sets](#hold-out)\n",
    "- [Additional resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting-underfitting'></a>\n",
    "\n",
    "## Overfitting and underfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://tomrobertshaw.net/img/2015/12/overfitting.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is wrong with the first model?**\n",
    "- The underfit model falls short of capturing the complexity of the \"true model\" of the data.\n",
    "\n",
    "**What is wrong with the third model?**\n",
    "- The overfit model is too complex and is modeling random noise in the dat.\n",
    "\n",
    "**Middle model is a good compromise.**\n",
    "- It approximates the complexity of the true model and does not model random noise in our sample as true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://image.slidesharecdn.com/nncollovcapaldo2013-131220052427-phpapp01/95/machine-learning-introduction-to-neural-networks-12-638.jpg?cb=1393073301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train-test-split\"></a>\n",
    "## Train-test split and model validation\n",
    "\n",
    "---\n",
    "\n",
    "So far we've focused on fitting the best model to our data. But is this the best model for our sample data or the best model overall? How do we know?\n",
    "\n",
    "In practice, we need to validate our model's ability to generalize to new data. One very popuplar method for performing model validation is by splitting our data into subsets: data that we *train* our model on, and data that we *test* our model on.\n",
    "\n",
    "The most basic type of \"hold-out\" validation is called **train-test split**. We split our data into two pieces:\n",
    "\n",
    "> **\"Training set\":** the subset of the data that we fit our model on.\n",
    "\n",
    "> **\"Testing set\":** the subset of the data that we evaluate the quality of our predictions on.\n",
    "\n",
    "\n",
    "**Test/train split benefits:**\n",
    "- Testing data can be a proxy for \"future\" data; for prediction-oriented models it is critical to make sure a model performing well on current data will likely perform well on future data.\n",
    "- Can help diagnose and avoid overfitting via model tuning.\n",
    "- Improve the quality of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross-val-k-fold'></a>\n",
    "\n",
    "## K-Fold cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "K-Fold cross-validation takes the idea of a single train-test split and expands this to *multiple tests* across different train-test splits of your data.\n",
    "\n",
    "For example, if you determine your training set will contain 80% of the data and your testing set will contain the other 20%, you could have 5 different 80/20 splits where the test set in each is different sets of observations. We have:\n",
    "- 5 (K=5) training sets\n",
    "- 5 (K=5) corresponding test sets\n",
    "\n",
    "**K-Fold cross-validation builds K models, one for each train-test pair, and evaluates those models on each respective test-set.**\n",
    "\n",
    "### K-Fold cross-validation visually\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\"a>\n",
    "\n",
    "---\n",
    "\n",
    "Cross-validation helps us understand how a model parameterization may perform in a variety of cases. The K-Fold cross-validation procedure can be described in pseudocode:\n",
    "\n",
    "```\n",
    "set k\n",
    "create k groups of rows in data\n",
    "\n",
    "for group i in k row groups:\n",
    "    test data is data[group i]\n",
    "    train data is data[all groups not i]\n",
    "    \n",
    "    fit model on train data\n",
    "    \n",
    "    score model on test data\n",
    "    \n",
    "evaluate mean of k model scores\n",
    "evaluate variance of k model scores\n",
    "```\n",
    "\n",
    "Odd case #1:\n",
    "> **When K=2**: This is equivalent to doing ***two*** mirror image 50-50 train-test splits.\n",
    "\n",
    "Odd case #2:\n",
    "> **When K=number of rows**: This is known as \"leave one out cross-validation\" or LOOCV. A model is built on all but one row and tested on the single held-out observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "## Train-test split with sklearn demonstration\n",
    "\n",
    "---\n",
    "\n",
    "Let's use sklearn to load everyone's favorite data set: the boston housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a heatmap of the correlation matrix\n",
    "\n",
    "Heatmaps are a great way to visually examine the correlational structure of your predictors. \n",
    "\n",
    "> Keep in mind that pearson correlation between non-dummy-coded categorical variables and other variables are invalid!\n",
    "\n",
    "Let's lift some code from GitHub Gist to plot a correlation heat map.\n",
    "\n",
    "[Ryan's Heatmap Cheat Sheet](https://gist.github.com/rddunlap23/02d4014907ffaf9cbdf49f4004f82ba6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single-predictor'></a>\n",
    "\n",
    "### Select a single predictor for a SLR\n",
    "\n",
    "The variable `AGE` appears to have a minor linear relationship with the target variable, `MEDV`.\n",
    "\n",
    "Let's select just `AGE` out of the data as a single column design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good practice to plot the variable against the target to confirm the relationship visually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-tts'></a>\n",
    "\n",
    "### sklearn's `train_test_split` function\n",
    "\n",
    "Train test split using sklearn is easy. Load the `train_test_split` function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "**Arguments**:\n",
    "- *arrays: Any number of arrays/matrices to split up into training and testing (they should be the same length).\n",
    "- `test_size`: an integer for exact size of the test subset or a float for a percentage\n",
    "- `train_size`: alternatively you can specify the training size\n",
    "- `stratify`: supply a vector to stratify the splitting by (more important in classification tasks)\n",
    "\n",
    "**Perform a 50-50 split of our `X` and `y`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could always split the data up manually. Here's an example for [this dataset](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) of manual splitting code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-on-train'></a>\n",
    "\n",
    "### Fit a linear regression on the training set\n",
    "\n",
    "Using the training `X` and training `y`, we can fit a linear regression with sklearn's `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='score-on-test'></a>\n",
    "\n",
    "### Calculate the $R^2$ score on the test data\n",
    "\n",
    "After we have our model constructed on the training set, we can evaluate how well our model performs on data it has no exposure to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare this to the model scored on the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-demo'></a>\n",
    "\n",
    "## K-Fold cross-validation demonstration\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now let's try out k-fold cross-validation. Again scikit-learn provides useful functions to do the heavy lifting. \n",
    "\n",
    "The function `cross_val_score` returns the $R^2$ for each test set. \n",
    "\n",
    "Alternatively, the function `cross_val_predict` returns the predicted values for each data point when it's in the testing slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neg-r2'></a>\n",
    "## Review: negative $R^2$ values\n",
    "\n",
    "---\n",
    "\n",
    "What does it mean to have a negative $R^2$?\n",
    "\n",
    "A negative $R^2$ only makes sense (and can only be found) when we are evaluating the $R^2$ score on data that the model was not fit on. If $R^2$ is evaluated for a model using the training data, *the minimum $R^2$ must be zero.* \n",
    "\n",
    "However, on a test set the $R^2$ **can** be negative. This means that the model performs so poorly on the testing set that you would have been better off just using the mean of the target from the training set as an estimate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hold-out'></a>\n",
    "\n",
    "## Hold-out sets\n",
    "\n",
    "---\n",
    "\n",
    "Hold-out sets are a version of train-test split. The concept of having a hold-out set is:\n",
    "1. **Split data into a large train and small test set. This small test set will be the \"hold-out\" set.**\n",
    "2. **For a set of different model parameterizations:**\n",
    "    1. **Set up the model.**\n",
    "    2. **Cross-validate the current model on the training data.**\n",
    "    3. **Save the model performance.**\n",
    "3. **Select the model that performed best using cross-validation on the training data.**\n",
    "4. **Perform a final test of that model on the original \"hold-out\" test set.**\n",
    "\n",
    "> **Note:** The \"hold-out\" method is more conservative, but also requires that you have more data. With smaller datasets it can be infeasable.\n",
    "\n",
    "The graphic below explains the hold-out method visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Cross-validation Example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py)\n",
    "- [Plotting Cross-Validated Predictions](http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html)\n",
    "- Examine this [academic paper](http://frostiebek.free.fr/docs/Machine%20Learning/validation-1.pdf) on the underpinnings of the holdout method, LOOVC, and kfolds\n",
    "- The sklearn [documentation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) on cross validation is strong\n",
    "- This [Stanford lesson](https://www.youtube.com/watch?v=_2ij6eaaSl0) on cross validation\n",
    "- This [blog post](http://www.win-vector.com/blog/2015/01/random-testtrain-split-is-not-always-enough/) on why TTS is not always enough\n",
    "- StackExchange [discussion](http://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio) on approximate TTS, validation set sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
