{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Gridsearching Hyperparameters\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), David Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/aYcCt2.jpg)\n",
    "\n",
    "### Learning Objective\n",
    "- Understand what the terms gridsearch and hyperparameter refer to.\n",
    "- Understand how to manually build a gridsearching procedure.\n",
    "- Apply sklearn's `GridSearchCV` object with basketball data to optimize a KNN model.\n",
    "- Practice using and evaluating attributes of the gridsearch object.\n",
    "- Understand the pitfalls of searching large hyperparameter spaces.\n",
    "- Practice the gridsearch procedure independently optimizing regularized logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [What is \"Gridsearching\"? What are \"hyperparameters\"?](#intro)\n",
    "- [Basketball data](#basketball-data)\n",
    "- [Fitting a default KNN](#fit-knn)\n",
    "- [Searching for the best hyperparameters](#searching)\n",
    "    - [Gridsearch pseudocode](#pseudocode)\n",
    "    - [Using `GridSearchCV`](#gscv)\n",
    "- [A caution on gridsearching](#caution)\n",
    "- [Independent practice: gridsearching regularization penalties with logistic regression](#practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## What is \"Gridsearching\"? What are \"hyperparameters\"?\n",
    "\n",
    "---\n",
    "\n",
    "Models often have specifications that can be set. For example, when we choose a linear regression, we may decide to add a penalty to the loss function such as the Ridge or the Lasso. Those penalties require the regularization strength, alpha, to be set. \n",
    "\n",
    "**Model parameters are called hyperparameters.**\n",
    "\n",
    "Hyperparameters are different than the parameters of the model resulting from a fit, such as the coefficients. The hyperparameters are set prior to the fit and determine the behavior of the model.\n",
    "\n",
    "There are often more than one kind of hyperparamter to set for a model. For example, in the KNN algorithm, we have a hyperparameter to set the number of neighbors. We also have a hyperparameter for the weights: uniform or distance? We want to know the *optimal* hyperparameter settings, the set that results in the best model evaluation. \n",
    "\n",
    "**The search for the optimal set of hyperparameters is called gridsearching.**\n",
    "\n",
    "Gridsearching gets its name from the fact that we are searching over a \"grid\" of parameters. For example, imagine the `n_neighbors` hyperparameters on the x-axis and `weights` on the y-axis, and we need to test all points on the grid.\n",
    "\n",
    "**Gridsearching uses cross-validation internally to evaluate the performance of each set of hyperparameters.** More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basketball-data'></a>\n",
    "\n",
    "## Basketball data\n",
    "\n",
    "---\n",
    "\n",
    "To explore the process of gridsearching over sets of hyperparameters, we will use some basketball data. The data below has statistics for 4 different seasons of NBA basketball: 2013-2016.\n",
    "- This data includes aggregate statistical data for each game. \n",
    "- The data of each game is aggregated by match for all players.\n",
    "- Scraped from http://www.basketball-refrence.com\n",
    "\n",
    "Many of the columns in the dataset represent the mean of a statistic across the last 10 games, for example. Non-target statistics are for *prior* games, they do not include information about player performance in the current game.\n",
    "\n",
    "**We are interested in predicting whether the home team will win the game or not.** This is a classification problem.\n",
    "\n",
    "\n",
    "### Load the data and create the target and predictor matrix\n",
    "- The target will be a binary column of whether the home team wins.\n",
    "- The predictors should be numeric statistics columns.\n",
    "\n",
    "Exclude these columns from the predictor matrix:\n",
    "\n",
    "    ['GameId','GameDate','GameTime','HostName',\n",
    "     'GuestName','total_score','total_line','game_line',\n",
    "     'winner','loser','host_wins','Season']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/basketball_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>GameId</th>\n",
       "      <th>GameDate</th>\n",
       "      <th>GameTime</th>\n",
       "      <th>HostName</th>\n",
       "      <th>GuestName</th>\n",
       "      <th>total_score</th>\n",
       "      <th>total_line</th>\n",
       "      <th>game_line</th>\n",
       "      <th>Host_HostRank</th>\n",
       "      <th>...</th>\n",
       "      <th>gPTS_avg10</th>\n",
       "      <th>gTS%_avg10</th>\n",
       "      <th>g3PAR_avg10</th>\n",
       "      <th>gFTr_avg10</th>\n",
       "      <th>gDRB%_avg10</th>\n",
       "      <th>gTRB%_avg10</th>\n",
       "      <th>gAST%_avg10</th>\n",
       "      <th>gSTL%_avg10</th>\n",
       "      <th>gBLK%_avg10</th>\n",
       "      <th>gDRtg_avg10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212090LAL</td>\n",
       "      <td>2012-12-09</td>\n",
       "      <td>6:30 pm</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>227.0</td>\n",
       "      <td>207.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>69.22</td>\n",
       "      <td>50.05</td>\n",
       "      <td>61.57</td>\n",
       "      <td>8.63</td>\n",
       "      <td>10.31</td>\n",
       "      <td>110.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100PHI</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>201.0</td>\n",
       "      <td>186.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>90.3</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>71.46</td>\n",
       "      <td>49.48</td>\n",
       "      <td>59.83</td>\n",
       "      <td>6.48</td>\n",
       "      <td>9.46</td>\n",
       "      <td>107.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100HOU</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>240.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>74.26</td>\n",
       "      <td>50.99</td>\n",
       "      <td>61.82</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.85</td>\n",
       "      <td>101.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110BRK</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>197.0</td>\n",
       "      <td>195.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>100.3</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>74.23</td>\n",
       "      <td>47.88</td>\n",
       "      <td>52.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>7.64</td>\n",
       "      <td>109.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110DET</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:30 pm</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "      <td>195.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>101.1</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>68.45</td>\n",
       "      <td>50.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>114.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season        GameId    GameDate GameTime            HostName  \\\n",
       "0    2013  201212090LAL  2012-12-09  6:30 pm  Los Angeles Lakers   \n",
       "1    2013  201212100PHI  2012-12-10  7:00 pm  Philadelphia 76ers   \n",
       "2    2013  201212100HOU  2012-12-10  7:00 pm     Houston Rockets   \n",
       "3    2013  201212110BRK  2012-12-11  7:00 pm       Brooklyn Nets   \n",
       "4    2013  201212110DET  2012-12-11  7:30 pm     Detroit Pistons   \n",
       "\n",
       "           GuestName  total_score  total_line  game_line  Host_HostRank  \\\n",
       "0          Utah Jazz        227.0       207.5        7.5             13   \n",
       "1    Detroit Pistons        201.0       186.5        5.5             13   \n",
       "2  San Antonio Spurs        240.0       212.0       -7.0             12   \n",
       "3    New York Knicks        197.0       195.5       -3.5             12   \n",
       "4     Denver Nuggets        195.0       203.5       -4.5             11   \n",
       "\n",
       "      ...      gPTS_avg10  gTS%_avg10  g3PAR_avg10  gFTr_avg10  gDRB%_avg10  \\\n",
       "0     ...            99.0      0.5206       0.2230      0.2981        69.22   \n",
       "1     ...            90.3      0.5077       0.2144      0.3095        71.46   \n",
       "2     ...           108.0      0.5915       0.2743      0.2518        74.26   \n",
       "3     ...           100.3      0.5473       0.3595      0.2544        74.23   \n",
       "4     ...           101.1      0.5605       0.2173      0.3177        68.45   \n",
       "\n",
       "   gTRB%_avg10  gAST%_avg10  gSTL%_avg10 gBLK%_avg10 gDRtg_avg10  \n",
       "0        50.05        61.57         8.63       10.31      110.87  \n",
       "1        49.48        59.83         6.48        9.46      107.91  \n",
       "2        50.99        61.82         8.30        6.85      101.41  \n",
       "3        47.88        52.07         9.31        7.64      109.24  \n",
       "4        50.40        56.33         7.67        7.83      114.86  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Season', u'GameId', u'GameDate', u'GameTime', u'HostName',\n",
       "       u'GuestName', u'total_score', u'total_line', u'game_line',\n",
       "       u'Host_HostRank', u'Host_GameRank', u'Guest_GuestRank',\n",
       "       u'Guest_GameRank', u'host_win_count', u'host_lose_count',\n",
       "       u'guest_win_count', u'guest_lose_count', u'game_behind', u'winner',\n",
       "       u'loser', u'host_place_streak', u'guest_place_streak', u'hq1_avg10',\n",
       "       u'hq2_avg10', u'hq3_avg10', u'hq4_avg10', u'hPace_avg10',\n",
       "       u'heFG%_avg10', u'hTOV%_avg10', u'hORB%_avg10', u'hFT/FGA_avg10',\n",
       "       u'hORtg_avg10', u'hFG_avg10', u'hFGA_avg10', u'hFG%_avg10',\n",
       "       u'h3P_avg10', u'h3PA_avg10', u'h3P%_avg10', u'hFT_avg10', u'hFTA_avg10',\n",
       "       u'hFT%_avg10', u'hORB_avg10', u'hDRB_avg10', u'hTRB_avg10',\n",
       "       u'hAST_avg10', u'hSTL_avg10', u'hBLK_avg10', u'hTOV_avg10',\n",
       "       u'hPF_avg10', u'hPTS_avg10', u'hTS%_avg10', u'h3PAR_avg10',\n",
       "       u'hFTr_avg10', u'hDRB%_avg10', u'hTRB%_avg10', u'hAST%_avg10',\n",
       "       u'hSTL%_avg10', u'hBLK%_avg10', u'hDRtg_avg10', u'gq1_avg10',\n",
       "       u'gq2_avg10', u'gq3_avg10', u'gq4_avg10', u'gPace_avg10',\n",
       "       u'geFG%_avg10', u'gTOV%_avg10', u'gORB%_avg10', u'gFT/FGA_avg10',\n",
       "       u'gORtg_avg10', u'gFG_avg10', u'gFGA_avg10', u'gFG%_avg10',\n",
       "       u'g3P_avg10', u'g3PA_avg10', u'g3P%_avg10', u'gFT_avg10', u'gFTA_avg10',\n",
       "       u'gFT%_avg10', u'gORB_avg10', u'gDRB_avg10', u'gTRB_avg10',\n",
       "       u'gAST_avg10', u'gSTL_avg10', u'gBLK_avg10', u'gTOV_avg10',\n",
       "       u'gPF_avg10', u'gPTS_avg10', u'gTS%_avg10', u'g3PAR_avg10',\n",
       "       u'gFTr_avg10', u'gDRB%_avg10', u'gTRB%_avg10', u'gAST%_avg10',\n",
       "       u'gSTL%_avg10', u'gBLK%_avg10', u'gDRtg_avg10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768, 96)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2013, 2014, 2015, 2016])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Utah Jazz\n",
       "1    Philadelphia 76ers\n",
       "2     San Antonio Spurs\n",
       "3       New York Knicks\n",
       "4        Denver Nuggets\n",
       "Name: winner, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.winner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['host_wins'] = (data.HostName == data.winner).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [c for c in data.columns if c not in ['GameId','GameDate','GameTime','HostName',\n",
    "                                                   'GuestName','total_score','total_line','game_line',\n",
    "                                                   'winner','loser','host_wins','Season']]\n",
    "X = data[predictors]\n",
    "y = data.host_wins.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and testing data\n",
    "- Test data should be the 2016 season data, train data will be the previous seasons.\n",
    "- Make sure to standardize your predictor matrix (easiest to do prior to splitting the data into training and testing)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "Xs = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = Xs[data.Season.isin([2013, 2014, 2015])]\n",
    "X_test = Xs[data.Season == 2016]\n",
    "\n",
    "y_train = y[data.Season.isin([2013, 2014, 2015])]\n",
    "y_test = y[data.Season == 2016]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-knn'></a>\n",
    "\n",
    "## Fitting the default KNN\n",
    "\n",
    "---\n",
    "\n",
    "Below we can fit a default `KNeighborsClassifier` to predict win vs. not on the training data, then score it on the testing data. \n",
    "\n",
    "Make sure to compare your score to the baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57258883248730963"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603045685279\n"
     ]
    }
   ],
   "source": [
    "print np.mean(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='searching'></a>\n",
    "\n",
    "## Searching for the best hyperparameters\n",
    "\n",
    "--\n",
    "\n",
    "Our default KNN performs quite poorly on the test data. But what if we changed the number of neighbors? The weighting? The distance metric?\n",
    "\n",
    "These are all hyperparameters of the KNN. How would we do this manually? We would need to evaluate on the training data the set of hyperparameters that perform best, and then use this set of hyperparameters to fit the final model and score on the testing set.\n",
    "\n",
    "<a id='pseudocode'></a>\n",
    "### Gridsearch pseudocode for our KNN\n",
    "\n",
    "```python\n",
    "accuracies = {}\n",
    "for k in neighbors_to_test:\n",
    "    for w in weightings_to_test:\n",
    "        for d in distance_metrics_to_test:\n",
    "            hyperparam_set = (k, w, d)\n",
    "            knn = KNeighborsClassifier(n_neighbors=n, weights=w, metric=d)\n",
    "            cv_accuracies = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "            accuracies[hyperparam_set] = np.mean(cv_accuracies)\n",
    "```\n",
    "\n",
    "In the pseudocode above, we would find the key in the dictionary (a hyperparameter set) that has the larget value (mean cross-validated accuracy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gscv'></a>\n",
    "### Using `GridSearchCV`\n",
    "\n",
    "This would be an annoying process to have to do manually. Luckily sklearn comes with a convenience class for performing gridsearch:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "The `GridSearchCV` has a handful of important arguments:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`estimator`** | The sklearn instance of the model to fit on |\n",
    "| **`param_grid`** | A dictionary where keys are hyperparameters for the model and values are lists of values to test |\n",
    "| **`cv`** | The number of internal cross-validation folds to run for each set of hyperparameters |\n",
    "| **`n_jobs`** | How many cores to use on your computer to run the folds (-1 means use all cores) |\n",
    "| **`verbose`** | How much output to display (0 is none, 1 is limited, 2 is printouts for every internal fit) |\n",
    "\n",
    "\n",
    "Below is an example for how one might set up the gridsearch for our KNN:\n",
    "\n",
    "```python\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[1,3,5,7,9],\n",
    "    'weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "knn_gridsearcher = GridSearchCV(KNeighborsClassifier(), knn_parameters, verbose=1)\n",
    "knn_gridsearcher.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Try out the sklearn gridsearch below on the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 3, 5, 9, 15, 21], 'metric': ['euclidean', 'manhattan'], 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':[1,3,5,9,15,21],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']\n",
    "}\n",
    "\n",
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, verbose=1)\n",
    "\n",
    "knn_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gs-results'></a>\n",
    "### Examining the results of the gridsearch\n",
    "\n",
    "Once the gridsearch has fit (this can take awhile!) we can pull out a variety of information and useful objects from the gridsearch object, stored as attributes:\n",
    "\n",
    "| Property | Use |\n",
    "| --- | ---|\n",
    "| **`results.param_grid`** | Displays parameters searched over. |\n",
    "| **`results.best_score_`** | Best mean cross-validated score achieved. |\n",
    "| **`results.best_estimator_`** | Reference to model with best score.  Is usable / callable. |\n",
    "| **`results.best_params_`** | The parameters that have been found to perform with the best score. |\n",
    "| **`results.grid_scores_`** | Display score attributes with corresponding parameters. | \n",
    "\n",
    "**Print out the best score found in the search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61264822134387353"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the set of hyperparameters that achieved the best score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 21, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign the best fit model (`best_estimator_`) to a variable and score it on the test data.**\n",
    "\n",
    "Compare this model to the baseline accuracy and your default KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62538071065989853"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_knn = knn_gridsearch.best_estimator_\n",
    "best_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.603045685279\n",
      "default KNN: 0.572588832487\n"
     ]
    }
   ],
   "source": [
    "print 'baseline:', np.mean(y_test)\n",
    "print 'default KNN:', knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='caution'></a>\n",
    "\n",
    "## A caution on gridsearching\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn models often have many options/hyperparameters with many different possible values. It may be tempting to search over a wide variety of them. In general, this is not wise.\n",
    "\n",
    "Remember that **gridsearch searches over all possible combinations of hyperparamters in the paramter dictionary!**\n",
    "\n",
    "The KNN model class takes a wider range of options during instantiation than we have explored. Imagine that we had this as our parameter dictionary:\n",
    "\n",
    "```python\n",
    "parameter_grid = {\n",
    "    'n_neighbors':range(1,151),\n",
    "    'weights':['uniform','distance',custom_function],\n",
    "    'algorithm':['ball_tree','kd_tree','brute','auto'],\n",
    "    'leaf_size':range(1,152),\n",
    "    'metric':['minkowski','euclidean'],\n",
    "    'p':[1,2]\n",
    "}\n",
    "```\n",
    "\n",
    "**How many different combinations will need to be tested?\n",
    "\n",
    "| Parameter | Potential Values | Unique Values |\n",
    "| --- | ---| ---: |\n",
    "| **n_neighbors** | int range 1-150 | 150 |\n",
    "| **weights** | strs:  \"uniform\", \"distance\" or user defined function | 3 |\n",
    "| **algorithm** | strs: \"ball_tree\", \"kd_tree\", \"brute\", \"auto\" | 4 |\n",
    "| **leaf_size** | int range 1-151 | 151 |\n",
    "| **metric** | str: \"minkowski\" or 'euclidean' type | 2 |\n",
    "| **p** | int: 1=manhattan_distance, 2= euclidean_distance | 2 |\n",
    "|| <br>_150 \\* 3 \\* 4 \\* 151 \\* 2 \\* 2 = n combinations_ <br><br>| _1,087,200_ |\n",
    "\n",
    "And that is over a million tests *before we even consider the number of cross-validation folds!*\n",
    "\n",
    "If you're not careful, gridsearching can quickly blow up. A lot of the hyperparameters we put in the dumb example above are either redundant or not useful.\n",
    "\n",
    "> **It is extremely important to understand what the hyperparameters do and think critically about what ranges are useful and relevant to your model!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "## Independent practice: gridsearch regularization penalties with logistic regression\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression models can also apply the Lasso and Ridge penalties. The `LogisticRegression` class takes these regularization-relevant hyperparameters:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`penalty`** | `'l1'` for Lasso, `'l2'` for Ridge |\n",
    "| **`solver`** | Must be set to `'liblinear'` for the Lasso penalty to work. |\n",
    "| **`C`** | The regularization strength. Equivalent to `1./alpha` |\n",
    "\n",
    "**You should:**\n",
    "1. Fit and validate the accuracy of a default logistic regression on the basketball data.\n",
    "- Perform a gridsearch over different regularization strengths and Lasso and Ridge penalties.\n",
    "- Compare the accuracy on the test set of your optimized logistic regression to the baseline accuracy and the default model.\n",
    "- Look at the best parameters found. What was chosen? What does this suggest about our data?\n",
    "- Look at the (non-zero, if Lasso was selected as best) coefficients and associated predictors for your optimized model. What appears to be the most important predictors of winning the game?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695431472081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603045685279\n"
     ]
    }
   ],
   "source": [
    "print np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default is already much better than KNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the parameters. Looking at C regularization strengths on a log scale.\n",
    "# This takes awhile...\n",
    "gs_params = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'solver':['liblinear'],\n",
    "    'C':np.logspace(-5,0,100)\n",
    "}\n",
    "\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), gs_params, cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   42.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([  1.00000e-05,   1.12332e-05, ...,   8.90215e-01,   1.00000e+00]), 'solver': ['liblinear']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66079770032339202"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best score on the training data:\n",
    "lr_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.0029836472402833404, 'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best parameters on the training data:\n",
    "lr_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lasso was chosen: this indicates that maybe unimportant (noise) variables\n",
    "# is more of an issue in our data than multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_lr = lr_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70050761421319796"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score it on the testing data:\n",
    "best_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slightly better than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "        'coef':best_lr.coef_[0],\n",
    "        'feature':X.columns\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_df['abs_coef'] = np.abs(coef_df.coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort by absolute value of coefficient (magnitude)\n",
    "coef_df.sort_values('abs_coef', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>feature</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.629919</td>\n",
       "      <td>game_behind</td>\n",
       "      <td>0.629919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.139348</td>\n",
       "      <td>hORtg_avg10</td>\n",
       "      <td>0.139348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.061373</td>\n",
       "      <td>hDRtg_avg10</td>\n",
       "      <td>0.061373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.061157</td>\n",
       "      <td>guest_lose_count</td>\n",
       "      <td>0.061157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.053149</td>\n",
       "      <td>gq1_avg10</td>\n",
       "      <td>0.053149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.032681</td>\n",
       "      <td>hTS%_avg10</td>\n",
       "      <td>0.032681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.027735</td>\n",
       "      <td>gDRtg_avg10</td>\n",
       "      <td>0.027735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.024479</td>\n",
       "      <td>gq3_avg10</td>\n",
       "      <td>0.024479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.024258</td>\n",
       "      <td>gORtg_avg10</td>\n",
       "      <td>0.024258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.024082</td>\n",
       "      <td>hBLK%_avg10</td>\n",
       "      <td>0.024082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.021980</td>\n",
       "      <td>hSTL%_avg10</td>\n",
       "      <td>0.021980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.019053</td>\n",
       "      <td>h3P_avg10</td>\n",
       "      <td>0.019053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.007436</td>\n",
       "      <td>gAST%_avg10</td>\n",
       "      <td>0.007436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.007070</td>\n",
       "      <td>g3PAR_avg10</td>\n",
       "      <td>0.007070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.003414</td>\n",
       "      <td>gTRB%_avg10</td>\n",
       "      <td>0.003414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.001265</td>\n",
       "      <td>hTOV_avg10</td>\n",
       "      <td>0.001265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000515</td>\n",
       "      <td>h3P%_avg10</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef           feature  abs_coef\n",
       "8   0.629919       game_behind  0.629919\n",
       "20  0.139348       hORtg_avg10  0.139348\n",
       "47 -0.061373       hDRtg_avg10  0.061373\n",
       "7   0.061157  guest_lose_count  0.061157\n",
       "48 -0.053149         gq1_avg10  0.053149\n",
       "39  0.032681        hTS%_avg10  0.032681\n",
       "84  0.027735       gDRtg_avg10  0.027735\n",
       "50  0.024479         gq3_avg10  0.024479\n",
       "57 -0.024258       gORtg_avg10  0.024258\n",
       "46  0.024082       hBLK%_avg10  0.024082\n",
       "45  0.021980       hSTL%_avg10  0.021980\n",
       "24  0.019053         h3P_avg10  0.019053\n",
       "81 -0.007436       gAST%_avg10  0.007436\n",
       "77 -0.007070       g3PAR_avg10  0.007070\n",
       "80 -0.003414       gTRB%_avg10  0.003414\n",
       "36 -0.001265        hTOV_avg10  0.001265\n",
       "26  0.000515        h3P%_avg10  0.000515"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show non-zero coefs and predictors\n",
    "coef_df[coef_df.coef != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game_behind, home team offensive rating average over the last ten games,\n",
    "# and to a lesser extend the defensive rating are predictors of the home\n",
    "# team winning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
